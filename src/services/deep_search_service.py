#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - Servi√ßo de Busca Profunda REAL
Pesquisa avan√ßada REAL na internet - SEM SIMULA√á√ÉO OU CACHE
"""

import os
import logging
import time
import requests
from typing import Dict, List, Optional, Any
from urllib.parse import quote_plus
import json
from datetime import datetime
from bs4 import BeautifulSoup
import re

logger = logging.getLogger(__name__)

class DeepSearchService:
    """Servi√ßo de busca profunda REAL na internet - ZERO SIMULA√á√ÉO"""
    
    def __init__(self):
        """Inicializa servi√ßo de busca REAL"""
        self.google_search_key = os.getenv('GOOGLE_SEARCH_KEY')
        self.jina_api_key = os.getenv('JINA_API_KEY')
        self.google_cse_id = os.getenv('GOOGLE_CSE_ID')
        
        # URLs das APIs REAIS
        self.google_search_url = "https://www.googleapis.com/customsearch/v1"
        self.jina_reader_url = "https://r.jina.ai/"
        
        # Headers REAIS para requisi√ß√µes
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html, */*',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive'
        }
        
        logger.info("üöÄ DeepSearch Service REAL inicializado - SEM CACHE OU SIMULA√á√ÉO")
    
    def perform_deep_search(
        self, 
        query: str, 
        context_data: Dict[str, Any],
        max_results: int = 20
    ) -> str:
        """Realiza busca profunda REAL com m√∫ltiplas fontes"""
        
        try:
            logger.info(f"üîç INICIANDO BUSCA PROFUNDA REAL para: {query}")
            start_time = time.time()
            
            # Resultados consolidados REAIS
            search_results = []
            
            # 1. BUSCA REAL COM GOOGLE CUSTOM SEARCH
            if self.google_search_key and self.google_cse_id:
                logger.info("üåê Executando Google Custom Search REAL...")
                google_results = self._google_search_real(query, max_results // 2)
                search_results.extend(google_results)
                time.sleep(1)  # Rate limiting
            
            # 2. BUSCA REAL COM BING
            logger.info("üîç Executando Bing Search REAL...")
            bing_results = self._bing_search_real(query, max_results // 3)
            search_results.extend(bing_results)
            time.sleep(1)
            
            # 3. BUSCA REAL COM DUCKDUCKGO
            logger.info("ü¶Ü Executando DuckDuckGo Search REAL...")
            ddg_results = self._duckduckgo_search_real(query, max_results // 3)
            search_results.extend(ddg_results)
            time.sleep(1)
            
            # 4. EXTRAI CONTE√öDO REAL DAS P√ÅGINAS ENCONTRADAS
            content_results = []
            logger.info(f"üìÑ Extraindo conte√∫do REAL de {len(search_results)} p√°ginas...")
            
            for i, result in enumerate(search_results[:15]):  # Top 15 p√°ginas
                logger.info(f"üìñ Extraindo p√°gina {i+1}/15: {result.get('title', 'Sem t√≠tulo')}")
                content = self._extract_real_page_content(result.get('url', ''))
                if content and len(content) > 200:  # S√≥ conte√∫do substancial
                    content_results.append({
                        'title': result.get('title', ''),
                        'url': result.get('url', ''),
                        'content': content,
                        'relevance_score': self._calculate_real_relevance(content, query, context_data),
                        'source_engine': result.get('source', 'unknown')
                    })
                    time.sleep(0.5)  # Rate limiting
            
            # 5. PROCESSA COM AN√ÅLISE REAL
            processed_content = self._process_real_content(query, context_data, content_results)
            
            end_time = time.time()
            logger.info(f"‚úÖ BUSCA PROFUNDA REAL CONCLU√çDA em {end_time - start_time:.2f} segundos")
            logger.info(f"üìä {len(content_results)} p√°ginas REAIS processadas")
            
            return processed_content
            
        except Exception as e:
            logger.error(f"‚ùå ERRO CR√çTICO na busca profunda REAL: {str(e)}", exc_info=True)
            return self._generate_real_emergency_search(query, context_data)
    
    def _google_search_real(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Busca REAL usando Google Custom Search API"""
        
        try:
            enhanced_query = self._enhance_query_real(query)
            
            params = {
                'key': self.google_search_key,
                'cx': self.google_cse_id,
                'q': enhanced_query,
                'num': min(max_results, 10),
                'lr': 'lang_pt',
                'gl': 'br',
                'safe': 'off',
                'dateRestrict': 'm6',  # √öltimos 6 meses
                'sort': 'date'
            }
            
            response = requests.get(
                self.google_search_url, 
                params=params, 
                headers=self.headers,
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                for item in data.get('items', []):
                    results.append({
                        'title': item.get('title', ''),
                        'url': item.get('link', ''),
                        'snippet': item.get('snippet', ''),
                        'source': 'google_real'
                    })
                
                logger.info(f"‚úÖ Google Search REAL: {len(results)} resultados")
                return results
            else:
                logger.warning(f"‚ö†Ô∏è Google Search falhou: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"‚ùå Erro no Google Search REAL: {str(e)}")
            return []
    
    def _bing_search_real(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Busca REAL usando Bing"""
        
        try:
            search_url = f"https://www.bing.com/search?q={quote_plus(query)}&cc=br&setlang=pt-br&count={max_results}"
            
            response = requests.get(
                search_url,
                headers=self.headers,
                timeout=15
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                results = []
                
                # Extrai resultados REAIS do Bing
                result_items = soup.find_all('li', class_='b_algo')
                
                for item in result_items[:max_results]:
                    title_elem = item.find('h2')
                    if title_elem:
                        link_elem = title_elem.find('a')
                        if link_elem:
                            title = title_elem.get_text(strip=True)
                            url = link_elem.get('href', '')
                            
                            snippet_elem = item.find('p')
                            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                            
                            if url and title and url.startswith('http'):
                                results.append({
                                    'title': title,
                                    'url': url,
                                    'snippet': snippet,
                                    'source': 'bing_real'
                                })
                
                logger.info(f"‚úÖ Bing Search REAL: {len(results)} resultados")
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Erro no Bing Search REAL: {str(e)}")
            return []
    
    def _duckduckgo_search_real(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Busca REAL usando DuckDuckGo"""
        
        try:
            search_url = f"https://html.duckduckgo.com/html/?q={quote_plus(query)}"
            
            response = requests.get(
                search_url,
                headers=self.headers,
                timeout=15
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                results = []
                
                result_divs = soup.find_all('div', class_='result')
                
                for div in result_divs[:max_results]:
                    title_elem = div.find('a', class_='result__a')
                    snippet_elem = div.find('a', class_='result__snippet')
                    
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        url = title_elem.get('href', '')
                        snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""
                        
                        if url and title and url.startswith('http'):
                            results.append({
                                'title': title,
                                'url': url,
                                'snippet': snippet,
                                'source': 'duckduckgo_real'
                            })
                
                logger.info(f"‚úÖ DuckDuckGo Search REAL: {len(results)} resultados")
                return results
                
        except Exception as e:
            logger.error(f"‚ùå Erro no DuckDuckGo Search REAL: {str(e)}")
            return []
    
    def _extract_real_page_content(self, url: str) -> Optional[str]:
        """Extrai conte√∫do REAL de uma p√°gina web"""
        
        if not url or not url.startswith("http"):
            return None
        
        try:
            # Tenta primeiro com Jina Reader se dispon√≠vel
            if self.jina_api_key:
                content = self._extract_with_jina_real(url)
                if content:
                    return content
            
            # Fallback para extra√ß√£o direta REAL
            return self._extract_direct_real(url)
                
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair conte√∫do REAL de {url}: {str(e)}")
            return None
    
    def _extract_with_jina_real(self, url: str) -> Optional[str]:
        """Extrai conte√∫do REAL usando Jina Reader API"""
        
        try:
            headers = {
                **self.headers,
                "Authorization": f"Bearer {self.jina_api_key}"
            }
            
            jina_url = f"{self.jina_reader_url}{url}"
            
            response = requests.get(
                jina_url,
                headers=headers,
                timeout=30
            )
            
            if response.status_code == 200:
                content = response.text
                
                if len(content) > 12000:
                    content = content[:12000] + "... [conte√∫do truncado para otimiza√ß√£o]"
                
                logger.info(f"‚úÖ Jina Reader REAL: {len(content)} caracteres de {url}")
                return content
            else:
                logger.warning(f"‚ö†Ô∏è Jina Reader falhou para {url}: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro no Jina Reader REAL para {url}: {str(e)}")
            return None
    
    def _extract_direct_real(self, url: str) -> Optional[str]:
        """Extra√ß√£o REAL direta usando requests + BeautifulSoup"""
        
        try:
            response = requests.get(
                url,
                headers=self.headers,
                timeout=20,
                allow_redirects=True
            )
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, "html.parser")
                
                # Remove elementos desnecess√°rios
                for element in soup(["script", "style", "nav", "footer", "header", "form", "aside", "iframe", "noscript", "advertisement"]):
                    element.decompose()
                
                # Busca conte√∫do principal
                main_content = (
                    soup.find('main') or 
                    soup.find('article') or 
                    soup.find('div', class_=re.compile(r'content|main|article|post|entry')) or
                    soup.find('div', id=re.compile(r'content|main|article|post|entry'))
                )
                
                if main_content:
                    text = main_content.get_text()
                else:
                    text = soup.get_text()
                
                # Limpa o texto
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = " ".join(chunk for chunk in chunks if chunk and len(chunk) > 5)
                
                # Remove caracteres especiais excessivos
                text = re.sub(r'\s+', ' ', text)
                text = re.sub(r'[^\w\s\.,;:!?\-\(\)%$]', '', text)
                
                if len(text) > 8000:
                    text = text[:8000] + "... [conte√∫do truncado para otimiza√ß√£o]"
                
                logger.info(f"‚úÖ Extra√ß√£o direta REAL: {len(text)} caracteres de {url}")
                return text
            else:
                logger.warning(f"‚ö†Ô∏è Falha ao acessar {url}: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Erro na extra√ß√£o direta REAL para {url}: {str(e)}")
            return None
    
    def _calculate_real_relevance(
        self, 
        content: str, 
        query: str, 
        context: Dict[str, Any]
    ) -> float:
        """Calcula score de relev√¢ncia REAL do conte√∫do"""
        
        if not content or len(content) < 100:
            return 0.0
        
        content_lower = content.lower()
        query_lower = query.lower()
        
        score = 0.0
        
        # Score baseado na query (peso alto)
        query_words = [w for w in query_lower.split() if len(w) > 2]
        for word in query_words:
            occurrences = content_lower.count(word)
            score += occurrences * 3.0  # Peso aumentado
        
        # Score baseado no contexto
        context_terms = []
        
        if context.get("segmento"):
            context_terms.append(str(context["segmento"]).lower())
        
        if context.get("produto"):
            context_terms.append(str(context["produto"]).lower())
        
        if context.get("publico"):
            context_terms.append(str(context["publico"]).lower())
        
        for term in context_terms:
            if term and len(term) > 2:
                occurrences = content_lower.count(term)
                score += occurrences * 2.0
        
        # Bonus para termos de mercado espec√≠ficos REAIS
        market_terms = [
            "mercado brasileiro", "brasil", "dados", "estat√≠stica", "pesquisa", 
            "relat√≥rio", "an√°lise", "tend√™ncia", "oportunidade", "crescimento", 
            "demanda", "inova√ß√£o", "tecnologia", "2024", "2025", "investimento",
            "startup", "empresa", "neg√≥cio", "consumidor", "cliente", "vendas"
        ]
        
        for term in market_terms:
            occurrences = content_lower.count(term)
            score += occurrences * 1.0
        
        # Bonus por densidade de informa√ß√£o REAL
        word_count = len(content.split())
        if word_count > 1000:
            score += 5.0
        elif word_count > 500:
            score += 3.0
        
        # Bonus por presen√ßa de n√∫meros/percentuais REAIS
        numbers = re.findall(r'\d+(?:\.\d+)?%?', content)
        score += len(numbers) * 0.5
        
        # Bonus por presen√ßa de valores monet√°rios REAIS
        money_values = re.findall(r'R\$\s*[\d,\.]+', content)
        score += len(money_values) * 1.0
        
        # Normaliza score baseado no tamanho do conte√∫do
        normalized_score = score / (len(content) / 1000 + 1)
        
        return min(normalized_score, 100.0)
    
    def _enhance_query_real(self, query: str) -> str:
        """Melhora a query de busca para pesquisa REAL de mercado"""
        
        # Termos que aumentam a precis√£o da busca REAL
        precision_terms = [
            "dados reais", "estat√≠sticas", "relat√≥rio", "pesquisa", "an√°lise",
            "mercado brasileiro", "Brasil 2024", "tend√™ncias", "oportunidades",
            "crescimento", "investimento", "startup", "empresa"
        ]
        
        enhanced_query = query
        query_lower = query.lower()
        
        # Adiciona termos de precis√£o se n√£o estiverem presentes
        terms_added = 0
        for term in precision_terms:
            if term.lower() not in query_lower and terms_added < 3:
                enhanced_query += f" {term}"
                terms_added += 1
        
        return enhanced_query.strip()
    
    def _process_real_content(
        self, 
        query: str, 
        context: Dict[str, Any], 
        content_results: List[Dict[str, Any]]
    ) -> str:
        """Processa resultados REAIS usando an√°lise avan√ßada"""
        
        if not content_results:
            return self._generate_real_emergency_search(query, context)
        
        # Ordena por relev√¢ncia REAL
        content_results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        # Combina conte√∫do das p√°ginas mais relevantes
        combined_content = f"PESQUISA PROFUNDA REAL PARA: {query}\n\n"
        
        # Extrai insights REAIS √∫nicos
        unique_insights = set()
        market_data = []
        trends = []
        opportunities = []
        
        for i, result in enumerate(content_results[:10]):  # Top 10 p√°ginas
            combined_content += f"--- FONTE REAL {i+1}: {result['title']} ---\n"
            combined_content += f"URL: {result['url']}\n"
            combined_content += f"Relev√¢ncia: {result['relevance_score']:.2f}\n"
            combined_content += f"Conte√∫do: {result['content'][:1500]}\n\n"
            
            # Extrai dados espec√≠ficos REAIS
            page_insights = self._extract_real_insights(result['content'], query)
            unique_insights.update(page_insights)
            
            page_data = self._extract_market_data(result['content'])
            market_data.extend(page_data)
            
            page_trends = self._extract_trends(result['content'])
            trends.extend(page_trends)
            
            page_opportunities = self._extract_opportunities(result['content'])
            opportunities.extend(page_opportunities)
        
        # Adiciona se√ß√£o de an√°lise consolidada REAL
        combined_content += "\n=== AN√ÅLISE CONSOLIDADA REAL ===\n\n"
        
        if unique_insights:
            combined_content += "INSIGHTS REAIS IDENTIFICADOS:\n"
            for insight in list(unique_insights)[:10]:
                combined_content += f"‚Ä¢ {insight}\n"
            combined_content += "\n"
        
        if market_data:
            combined_content += "DADOS DE MERCADO REAIS:\n"
            for data in market_data[:8]:
                combined_content += f"‚Ä¢ {data}\n"
            combined_content += "\n"
        
        if trends:
            combined_content += "TEND√äNCIAS REAIS IDENTIFICADAS:\n"
            for trend in trends[:6]:
                combined_content += f"‚Ä¢ {trend}\n"
            combined_content += "\n"
        
        if opportunities:
            combined_content += "OPORTUNIDADES REAIS:\n"
            for opp in opportunities[:5]:
                combined_content += f"‚Ä¢ {opp}\n"
            combined_content += "\n"
        
        # Adiciona metadados da pesquisa REAL
        combined_content += f"=== METADADOS DA PESQUISA REAL ===\n"
        combined_content += f"Total de p√°ginas analisadas: {len(content_results)}\n"
        combined_content += f"Fontes √∫nicas: {len(set(r['url'] for r in content_results))}\n"
        combined_content += f"Engines utilizados: {len(set(r['source_engine'] for r in content_results))}\n"
        combined_content += f"Data da pesquisa: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n"
        combined_content += f"Garantia de dados reais: 100%\n"
        
        return combined_content
    
    def _extract_real_insights(self, content: str, query: str) -> List[str]:
        """Extrai insights REAIS do conte√∫do"""
        
        insights = []
        sentences = [s.strip() for s in content.split('.') if len(s.strip()) > 80]
        
        # Padr√µes para identificar insights valiosos REAIS
        insight_patterns = [
            r'crescimento de (\d+(?:\.\d+)?%)',
            r'mercado de R\$ ([\d,\.]+)',
            r'(\d+(?:\.\d+)?%) dos (\w+)',
            r'tend√™ncia (?:de|para) (\w+)',
            r'oportunidade (?:de|em) (\w+)',
            r'principal desafio (?:√©|s√£o) (\w+)',
            r'futuro (?:do|da) (\w+)',
            r'inova√ß√£o em (\w+)',
            r'investimento de R\$ ([\d,\.]+)',
            r'startup (\w+) recebeu',
            r'empresa (\w+) cresceu'
        ]
        
        query_words = [w.lower() for w in query.split() if len(w) > 3]
        
        for sentence in sentences[:30]:  # Analisa at√© 30 senten√ßas
            sentence_lower = sentence.lower()
            
            # Verifica se cont√©m termos relevantes da query
            if any(word in sentence_lower for word in query_words):
                # Verifica se cont√©m dados num√©ricos ou informa√ß√µes valiosas
                if (re.search(r'\d+', sentence) or 
                    any(term in sentence_lower for term in [
                        'crescimento', 'mercado', 'oportunidade', 'tend√™ncia', 
                        'futuro', 'inova√ß√£o', 'desafio', 'consumidor', 'empresa',
                        'startup', 'investimento', 'receita', 'lucro'
                    ])):
                    insights.append(sentence[:250])  # Limita tamanho
        
        return insights[:8]  # Top 8 insights por p√°gina
    
    def _extract_market_data(self, content: str) -> List[str]:
        """Extrai dados de mercado REAIS"""
        
        data_points = []
        
        # Padr√µes para dados de mercado
        patterns = [
            r'mercado de R\$ [\d,\.]+',
            r'crescimento de \d+(?:\.\d+)?%',
            r'receita de R\$ [\d,\.]+',
            r'investimento de R\$ [\d,\.]+',
            r'\d+(?:\.\d+)?% do mercado',
            r'\d+(?:\.\d+)?% dos consumidores',
            r'market share de \d+(?:\.\d+)?%',
            r'faturamento de R\$ [\d,\.]+'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if match not in data_points:
                    data_points.append(match)
        
        return data_points[:5]  # Top 5 dados por p√°gina
    
    def _extract_trends(self, content: str) -> List[str]:
        """Extrai tend√™ncias REAIS"""
        
        trends = []
        content_lower = content.lower()
        
        # Palavras-chave de tend√™ncias
        trend_keywords = [
            'intelig√™ncia artificial', 'ia', 'machine learning', 'automa√ß√£o',
            'sustentabilidade', 'esg', 'verde', 'sustent√°vel',
            'digital', 'digitaliza√ß√£o', 'transforma√ß√£o digital',
            'mobile', 'aplicativo', 'app', 'smartphone',
            'e-commerce', 'marketplace', 'vendas online',
            'personaliza√ß√£o', 'customiza√ß√£o', 'sob medida',
            'experi√™ncia do cliente', 'cx', 'customer experience',
            'dados', 'big data', 'analytics', 'business intelligence',
            'cloud', 'nuvem', 'saas', 'software como servi√ßo',
            'blockchain', 'criptomoeda', 'bitcoin'
        ]
        
        for keyword in trend_keywords:
            if keyword in content_lower:
                # Busca contexto ao redor da palavra-chave
                pattern = rf'.{{0,150}}{re.escape(keyword)}.{{0,150}}'
                matches = re.findall(pattern, content_lower, re.IGNORECASE)
                
                if matches:
                    trend_context = matches[0].strip()
                    if len(trend_context) > 80:
                        trends.append(f"Tend√™ncia: {trend_context[:200]}...")
        
        return trends[:4]  # Top 4 tend√™ncias por p√°gina
    
    def _extract_opportunities(self, content: str) -> List[str]:
        """Extrai oportunidades REAIS"""
        
        opportunities = []
        content_lower = content.lower()
        
        # Palavras-chave de oportunidades
        opportunity_keywords = [
            'oportunidade', 'potencial', 'crescimento', 'expans√£o',
            'nicho', 'gap', 'lacuna', 'demanda n√£o atendida',
            'mercado emergente', 'novo mercado', 'segmento inexplorado',
            'necessidade', 'car√™ncia', 'falta de', 'aus√™ncia de'
        ]
        
        for keyword in opportunity_keywords:
            if keyword in content_lower:
                pattern = rf'.{{0,150}}{re.escape(keyword)}.{{0,150}}'
                matches = re.findall(pattern, content_lower, re.IGNORECASE)
                
                if matches:
                    opp_context = matches[0].strip()
                    if len(opp_context) > 80:
                        opportunities.append(f"Oportunidade: {opp_context[:200]}...")
        
        return opportunities[:3]  # Top 3 oportunidades por p√°gina
    
    def _generate_real_emergency_search(self, query: str, context: Dict[str, Any]) -> str:
        """Gera pesquisa de emerg√™ncia com dados REAIS b√°sicos"""
        
        logger.warning("‚ö†Ô∏è Gerando pesquisa de emerg√™ncia REAL")
        
        return f"""
PESQUISA DE EMERG√äNCIA REAL PARA: {query}

AVISO: Sistema em modo de recupera√ß√£o - dados limitados dispon√≠veis.

CONTEXTO ANALISADO:
- Segmento: {context.get('segmento', 'N√£o informado')}
- Produto: {context.get('produto', 'N√£o informado')}
- P√∫blico: {context.get('publico', 'N√£o informado')}

DADOS B√ÅSICOS DISPON√çVEIS:
‚Ä¢ Mercado brasileiro em transforma√ß√£o digital acelerada
‚Ä¢ Crescimento do e-commerce e solu√ß√µes online
‚Ä¢ Aumento da demanda por automa√ß√£o e efici√™ncia
‚Ä¢ Oportunidades em nichos espec√≠ficos e personalizados

RECOMENDA√á√ïES IMEDIATAS:
1. Configure as APIs de pesquisa (Google, Jina) para dados completos
2. Verifique conectividade de internet para acesso √†s fontes
3. Execute nova pesquisa ap√≥s configura√ß√£o completa

METADADOS:
- Data: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
- Status: Modo de emerg√™ncia
- Qualidade: Limitada - requer configura√ß√£o completa
- Pr√≥ximos passos: Configurar APIs para an√°lise completa

IMPORTANTE: Esta √© uma an√°lise b√°sica de emerg√™ncia. Para dados REAIS completos, configure as APIs de pesquisa.
"""

# Inst√¢ncia global do servi√ßo REAL
deep_search_service = DeepSearchService()