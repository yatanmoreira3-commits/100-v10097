#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v2.0 - AI Manager com Sistema de Fallback
Gerenciador inteligente de m√∫ltiplas IAs com fallback autom√°tico
"""

import os
import logging
import time
import json
from typing import Dict, List, Optional, Any
import requests

# Imports condicionais para os clientes de IA
try:
    import google.generativeai as genai
    HAS_GEMINI = True
except ImportError:
    HAS_GEMINI = False

try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

try:
    from services.groq_client import groq_client
    HAS_GROQ_CLIENT = True
except ImportError:
    HAS_GROQ_CLIENT = False

logger = logging.getLogger(__name__)

class AIManager:
    """Gerenciador de IAs com sistema de fallback autom√°tico"""

    def __init__(self):
        """Inicializa o AI Manager com m√∫ltiplos provedores"""
        self.providers = {}
        self.fallback_order = []
        self.provider_stats = {}
        self.disabled_providers = set()

        self._initialize_providers()

        logger.info(f"ü§ñ AI Manager inicializado com {len(self.providers)} provedores dispon√≠veis.")

    def _initialize_providers(self):
        """Inicializa todos os provedores de IA com base nas chaves de API dispon√≠veis."""

        # Inicializa Gemini
        if HAS_GEMINI:
            try:
                gemini_key = os.getenv('GEMINI_API_KEY')
                if gemini_key:
                    genai.configure(api_key=gemini_key)
                    # Tenta instanciar um modelo espec√≠fico para verificar a chave
                    # Se falhar aqui, considera o provedor indispon√≠vel
                    try:
                        model = genai.GenerativeModel("gemini-2.0-flash-exp")
                        # Teste r√°pido para ver se a API responde
                        model.generate_content("teste", generation_config={"max_output_tokens": 5})
                        self.providers['gemini'] = {
                            'client': model,
                            'available': True,
                            'priority': 1,
                            'model': 'gemini-2.0-flash-exp',
                            'max_errors': 2,
                            'consecutive_failures': 0
                        }
                        self.fallback_order.append('gemini')
                        logger.info("‚úÖ Gemini 2.5 Pro (gemini-2.0-flash-exp) inicializado como MODELO PRIM√ÅRIO")
                    except Exception as gemini_test_e:
                        logger.warning(f"‚ö†Ô∏è Gemini 2.5 Pro n√£o p√¥de ser instanciado ou testado: {str(gemini_test_e)}")
                        self.providers['gemini'] = {'available': False, 'error': str(gemini_test_e)}

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Falha ao configurar Gemini: {str(e)}")
                self.providers['gemini'] = {'available': False, 'error': str(e)}
        else:
            logger.warning("‚ö†Ô∏è Biblioteca 'google-generativeai' n√£o instalada.")
            self.providers['gemini'] = {'available': False, 'error': 'Biblioteca n√£o instalada'}


        # Inicializa OpenAI
        if HAS_OPENAI:
            try:
                openai_key = os.getenv('OPENAI_API_KEY')
                if openai_key:
                    # Tenta instanciar o cliente OpenAI
                    client = openai.OpenAI(api_key=openai_key)
                    # Teste r√°pido para ver se a API responde
                    client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "teste"}],
                        max_tokens=5
                    )
                    self.providers['openai'] = {
                        'client': client,
                        'available': True,
                        'priority': 3,
                        'model': 'gpt-3.5-turbo',
                        'max_errors': 2,
                        'consecutive_failures': 0
                    }
                    self.fallback_order.append('openai')
                    logger.info("‚úÖ OpenAI (gpt-3.5-turbo) inicializado com sucesso")
            except Exception as e:
                logger.info(f"‚ÑπÔ∏è OpenAI n√£o dispon√≠vel: {str(e)}")
                self.providers['openai'] = {'available': False, 'error': str(e)}
        else:
            logger.info("‚ÑπÔ∏è Biblioteca 'openai' n√£o instalada.")
            self.providers['openai'] = {'available': False, 'error': 'Biblioteca n√£o instalada'}

        # Inicializa Groq
        try:
            if HAS_GROQ_CLIENT and groq_client and groq_client.is_enabled():
                # Teste r√°pido para ver se a API responde
                groq_client.generate("teste", max_tokens=5)
                self.providers['groq'] = {
                    'client': groq_client,
                    'available': True,
                    'priority': 2,
                    'model': 'llama3-70b-8192',
                    'max_errors': 2,
                    'consecutive_failures': 0
                }
                self.fallback_order.append('groq')
                logger.info("‚úÖ Groq (llama3-70b-8192) inicializado com sucesso")
            else:
                logger.info("‚ÑπÔ∏è Groq client n√£o configurado ou n√£o habilitado")
                self.providers['groq'] = {'available': False, 'error': 'Cliente n√£o configurado ou desabilitado'}
        except Exception as e:
            logger.info(f"‚ÑπÔ∏è Groq n√£o dispon√≠vel: {str(e)}")
            self.providers['groq'] = {'available': False, 'error': str(e)}


        # Inicializa HuggingFace
        try:
            hf_key = os.getenv('HUGGINGFACE_API_KEY')
            if hf_key:
                # Cria um cliente mock para HuggingFace, pois a l√≥gica est√° em _generate_with_huggingface
                self.providers['huggingface'] = {
                    'client': {'api_key': hf_key, 'base_url': 'https://api-inference.huggingface.co/models/'},
                    'available': True,
                    'priority': 4,
                    'models': ["HuggingFaceH4/zephyr-7b-beta", "google/flan-t5-base"],
                    'current_model_index': 0,
                    'max_errors': 3,
                    'consecutive_failures': 0
                }
                # Teste r√°pido de conex√£o/autentica√ß√£o com HuggingFace
                url = f"{self.providers['huggingface']['client']['base_url']}HuggingFaceH4/zephyr-7b-beta"
                headers = {"Authorization": f"Bearer {hf_key}"}
                requests.post(url, headers=headers, json={"inputs": "teste"}, timeout=10)

                self.fallback_order.append('huggingface')
                logger.info("‚úÖ HuggingFace inicializado com sucesso")
        except Exception as e:
            logger.info(f"‚ÑπÔ∏è HuggingFace n√£o dispon√≠vel: {str(e)}")
            self.providers['huggingface'] = {'available': False, 'error': str(e)}


        # Garante que a ordem de fallback seja consistente e inclua todos os provedores
        # Ordena por prioridade definida, e para prioridades iguais, mant√©m a ordem de inicializa√ß√£o
        self.fallback_order.sort(key=lambda p: self.providers.get(p, {}).get('priority', float('inf')))


    def _register_success(self, provider_name: str):
        """Registra um sucesso para um provedor."""
        if provider_name in self.providers:
            self.providers[provider_name]['consecutive_failures'] = 0
            logger.debug(f"‚úÖ Sucesso registrado para {provider_name}")

    def _register_failure(self, provider_name: str, error: Exception):
        """Registra uma falha para um provedor e o desabilita se os erros excederem o limite."""
        if provider_name in self.providers:
            self.providers[provider_name]['consecutive_failures'] = self.providers[provider_name].get('consecutive_failures', 0) + 1
            self.providers[provider_name]['error'] = str(error) # Armazena o √∫ltimo erro

            if self.providers[provider_name]['consecutive_failures'] >= self.providers[provider_name].get('max_errors', 2):
                self.disabled_providers.add(provider_name)
                self.providers[provider_name]['available'] = False # Marca como indispon√≠vel
                logger.warning(f"‚ö†Ô∏è Provedor {provider_name} desabilitado temporariamente ap√≥s {self.providers[provider_name]['consecutive_failures']} falhas consecutivas.")


    def generate_content(self, prompt: str, max_tokens: int = 2000, temperature: float = 0.7) -> str:
        """Gera conte√∫do usando os provedores de IA dispon√≠veis"""
        try:
            # Tenta cada provedor na ordem de fallback
            for provider_name in self.fallback_order:
                if provider_name in self.disabled_providers:
                    continue

                provider_info = self.providers.get(provider_name)
                if not provider_info or not provider_info.get('available'):
                    continue

                try:
                    logger.info(f"üîÑ Tentando gera√ß√£o com {provider_name.upper()}")
                    client = provider_info.get('client')

                    response_text = None
                    if provider_name == 'gemini' and client:
                        response = client.generate_content(
                            prompt,
                            generation_config={"temperature": temperature, "max_output_tokens": min(max_tokens, 8192)},
                            safety_settings=[{"category": c, "threshold": "BLOCK_NONE"} for c in ["HARM_CATEGORY_HARASSMENT", "HARM_CATEGORY_HATE_SPEECH", "HARM_CATEGORY_SEXUALLY_EXPLICIT", "HARM_CATEGORY_DANGEROUS_CONTENT"]]
                        )
                        response_text = response.text
                    elif provider_name == 'groq' and client:
                        response = client.generate(prompt, max_tokens=min(max_tokens, 8192), temperature=temperature)
                        response_text = response
                    elif provider_name == 'openai' and client:
                        response = client.chat.completions.create(
                            model=provider_info.get('model', 'gpt-3.5-turbo'),
                            messages=[
                                {"role": "system", "content": "Voc√™ √© um especialista em an√°lise de mercado ultra-detalhada."},
                                {"role": "user", "content": prompt}
                            ],
                            max_tokens=min(max_tokens, 4096),
                            temperature=temperature
                        )
                        response_text = response.choices[0].message.content
                    elif provider_name == 'huggingface' and client:
                        # L√≥gica espec√≠fica para HuggingFace
                        model_index = provider_info.get('current_model_index', 0)
                        models = provider_info.get('models', [])
                        
                        # Tenta rotacionar modelos para encontrar um que responda
                        for i in range(len(models)):
                            current_model_idx = (model_index + i) % len(models)
                            model = models[current_model_idx]
                            provider_info['current_model_index'] = current_model_idx # Atualiza √≠ndice para pr√≥xima tentativa

                            try:
                                url = f"{client['base_url']}{model}"
                                headers = {"Authorization": f"Bearer {client['api_key']}"}
                                payload = {"inputs": prompt, "parameters": {"max_new_tokens": min(max_tokens, 1024), "temperature": temperature}}
                                
                                hf_response = requests.post(url, headers=headers, json=payload, timeout=60)

                                if hf_response.status_code == 200:
                                    res_json = hf_response.json()
                                    generated_text = res_json[0].get("generated_text", "")
                                    if generated_text:
                                        response_text = generated_text
                                        logger.info(f"‚úÖ HuggingFace ({model}) gerou {len(response_text)} caracteres")
                                        break # Sai do loop de modelos se obteve sucesso
                                elif hf_response.status_code == 503:
                                    logger.warning(f"‚ö†Ô∏è Modelo HuggingFace {model} est√° carregando (503), tentando pr√≥ximo...")
                                    continue
                                else:
                                    logger.warning(f"‚ö†Ô∏è Erro {hf_response.status_code} no modelo HuggingFace {model}: {hf_response.text}")
                                    continue
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Erro ao chamar modelo HuggingFace {model}: {e}")
                                continue
                        
                        if response_text is None:
                            raise Exception("Todos os modelos HuggingFace falharam ap√≥s tentativas.")

                    if response_text and len(response_text.strip()) > 10:
                        logger.info(f"‚úÖ {provider_name.upper()} gerou conte√∫do com sucesso.")
                        self._register_success(provider_name)
                        return response_text.strip()
                    else:
                        logger.warning(f"‚ö†Ô∏è {provider_name.upper()} retornou resposta vazia ou muito curta.")
                        raise Exception("Resposta vazia ou muito curta do provedor.")

                except Exception as e:
                    logger.error(f"‚ùå Erro ao usar provedor {provider_name.upper()}: {e}")
                    self._register_failure(provider_name, e)
                    continue # Tenta o pr√≥ximo provedor na lista de fallback

            # Se todos os provedores na lista de fallback falharem
            logger.warning("‚ö†Ô∏è Todos os provedores de IA falharam. Retornando resposta b√°sica.")
            return self._generate_basic_response(prompt)

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico no AI Manager: {e}")
            return self._generate_basic_response(prompt)


    def _generate_basic_response(self, prompt: str) -> str:
        """Gera resposta b√°sica quando todos os provedores falham"""

        # An√°lise b√°sica do prompt para resposta contextual
        if 'driver' in prompt.lower() or 'dire√ß√£o' in prompt.lower() or 'navega√ß√£o' in prompt.lower():
            return '''[
                {"nome": "Crescimento Seguro", "gatilho_central": "Medo de falha", "definicao_visceral": "Superar obst√°culos com confian√ßa"},
                {"nome": "Potencial Desbloqueado", "gatilho_central": "Desejo de crescimento", "definicao_visceral": "Liberar capacidades ocultas"},
                {"nome": "Dire√ß√£o Clara", "gatilho_central": "Incerteza", "definicao_visceral": "Encontrar caminho para sucesso"}
            ]'''

        elif 'prova' in prompt.lower() or 'visual' in prompt.lower() or 'evid√™ncia' in prompt.lower():
            return '''[
                {"nome": "Prova de Urg√™ncia", "categoria": "urgencia", "objetivo": "Criar senso de urg√™ncia"},
                {"nome": "Prova Social", "categoria": "social", "objetivo": "Valida√ß√£o por pares"},
                {"nome": "Prova de Autoridade", "categoria": "autoridade", "objetivo": "Demonstrar expertise"}
            ]'''

        elif 'obje√ß√£o' in prompt.lower() or 'objection' in prompt.lower() or 'impedimento' in prompt.lower():
            return '''[
                {"objecao": "N√£o tenho tempo", "categoria": "tempo", "resposta": "Foque no valor do tempo"},
                {"objecao": "Muito caro", "categoria": "dinheiro", "resposta": "Mostre o ROI"},
                {"objecao": "Preciso pensar", "categoria": "necessidade", "resposta": "Crie urg√™ncia"}
            ]'''

        elif 'pitch' in prompt.lower() or 'apresenta√ß√£o' in prompt.lower():
            return '''{
                "fases": [
                    {"fase": "quebra", "objetivo": "Quebrar ilus√£o", "script": "A realidade √© diferente do que parece"},
                    {"fase": "exposicao", "objetivo": "Mostrar problema", "script": "Aqui est√° o verdadeiro desafio"},
                    {"fase": "solucao", "objetivo": "Apresentar solu√ß√£o", "script": "Esta √© a resposta"}
                ]
            }'''

        elif 'avatar' in prompt.lower() or 'persona' in prompt.lower():
            return '''
            Avatar Empresarial:
            - Perfil: Empreendedor entre 35-45 anos
            - Dores: Sobrecarga, medo de falha, dificuldade para delegar
            - Desejos: Crescimento sustent√°vel, liberdade financeira, reconhecimento
            - Medos: Perder controle, falhar, n√£o ser visto como l√≠der
            '''

        elif 'previs√£o' in prompt.lower() or 'futuro' in prompt.lower() or 'tend√™ncias' in prompt.lower():
            return '''{
                "cenarios": {
                    "curto_prazo": {"tendencias": ["Digitaliza√ß√£o", "Automa√ß√£o"], "oportunidades": ["Nichos espec√≠ficos"]},
                    "medio_prazo": {"tendencias": ["IA mainstream", "Sustentabilidade"], "oportunidades": ["Parcerias estrat√©gicas"]},
                    "longo_prazo": {"tendencias": ["Economia digital"], "oportunidades": ["Mercados globais"]}
                }
            }'''
        
        elif 'c√≥digo' in prompt.lower() or 'excluir' in prompt.lower() or 'remover' in prompt.lower():
            return '''
            Recomenda√ß√µes de C√≥digo para Exclus√£o:
            - Fun√ß√µes redundantes ou n√£o utilizadas.
            - M√≥dulos com funcionalidade obsoleta.
            - Configura√ß√µes de API que n√£o est√£o mais em uso.
            - Arquivos de log ou tempor√°rios de longa data.
            - Bibliotecas desatualizadas sem planos de atualiza√ß√£o.
            '''

        else:
            return f"An√°lise b√°sica para: {prompt[:100]}... - Sistema funcionando em modo b√°sico. Configure APIs para an√°lise completa."


    def get_provider_status(self) -> Dict[str, str]:
        """Retorna status de todos os provedores"""
        status = {}
        for provider_name, provider_info in self.providers.items():
            if provider_info.get('available', False):
                status[provider_name] = "available"
            else:
                status[provider_name] = f"unavailable ({provider_info.get('error', 'N/A')})"
        return status

# Inst√¢ncia global
ai_manager = AIManager()